
# Customer Churn Prediction

A machine learning and deep learning project to predict customer churn for a telecommunications company â€” complete with a Streamlit web application for real-time predictions.

---

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Dataset](#dataset)
- [Key Findings](#key-findings)
- [Workflow](#workflow)
- [ML Models & Results](#ml-models--results)
- [Deep Learning Model](#deep-learning-model)
- [Model Comparison](#model-comparison)
- [Streamlit App](#streamlit-app)
- [Requirements](#requirements)
- [How to Run](#how-to-run)
- [Project Structure](#project-structure)
- [Business Impact](#business-impact)

---

## ğŸ” Overview

Customer churn is one of the most costly problems in the telecom industry. This project builds both a classical machine learning pipeline and a deep learning model (ANN) to identify customers at risk of canceling their subscription. The final model is deployed as an interactive **Streamlit web app** that allows real-time churn predictions with risk level classification.

---

## ğŸ“Š Dataset

**Source:** [Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)

| Property | Detail |
|---|---|
| Records | 7,043 customers |
| Features | 20 columns |
| Target | `Churn` (Yes / No) |
| Class Split | 73.5% No Churn / 26.5% Churn |

---

## ğŸ’¡ Key Findings

> Insights discovered during exploratory data analysis

- ğŸ“ƒ **Contract Type** â€” Month-to-month customers churn at drastically higher rates than annual or two-year contract holders
- ğŸŒ **Internet Service** â€” Fiber optic users show significantly higher churn compared to DSL users
- ğŸ’³ **Payment Method** â€” Customers paying via electronic check are the highest-risk group
- ğŸ”’ **Online Security** â€” Customers without online security are far more likely to churn

---

## âš™ï¸ Workflow

```
Data Loading & EDA
       â†“
Feature Encoding (Label Encoding)
       â†“
Correlation Analysis & Feature Selection
       â†“
Train/Test Split (80/20)
       â†“
Class Balancing (SMOTE)
       â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      ML Pipeline          DL Pipeline
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 Model Benchmarking      StandardScaler
  (11 classifiers)             â†“
         â†“               SMOTE Balancing
  Hyperparameter               â†“
  Tuning (GridSearchCV)   ANN (Keras)
         â†“               64â†’32â†’16â†’1
  Threshold Opt.         Dropout + Adam
      (0.3)              EarlyStopping
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                â†“
  Final Model Export (.pkl / .keras)
                â†“
       Power BI Integration
                â†“
       Streamlit Web App ğŸš€
```

---

## ğŸ¤– ML Models & Results

### Cross-Validation Accuracy (SMOTE balanced)

| Model | CV Accuracy |
|---|---|
| âœ… Random Forest | 83.99% |
| âœ… LightGBM | 83.58% |
| âœ… XGBoost | 83.46% |
| âœ… CatBoost | 83.35% |
| âœ… Gradient Boosting | 81.91% |
| AdaBoost | 80.50% |
| Logistic Regression | 79.42% |
| Decision Tree | 78.02% |
| BernoulliNB | 76.78% |
| KNN | 76.53% |
| SVM | 57.08% |

### After Hyperparameter Tuning (GridSearchCV)

| Model | Best Score | Best Params |
|---|---|---|
| Random Forest | 84.15% | `n_estimators=300, max_depth=None` |
| CatBoost | 83.91% | `depth=8, iterations=500, lr=0.05` |
| LightGBM | 83.70% | `n_estimators=500, lr=0.05` |
| XGBoost | 83.51% | `n_estimators=150, max_depth=6, lr=0.1` |

### ğŸ† Selected ML Model â€” LightGBM (Test Set Performance)

**Threshold adjusted to 0.3** to maximize recall on the churn class.

| Metric | No Churn | Churn |
|---|---|---|
| Precision | 0.90 | 0.52 |
| Recall | 0.75 | **0.78** |
| F1-Score | 0.82 | 0.63 |
| Accuracy | | **75%** |
| **AUC** | | **0.834** |

---

## ğŸ§  Deep Learning Model â€” ANN

A separate ANN was built using TensorFlow/Keras on the same preprocessed and SMOTE-balanced data, with StandardScaler applied before training.

### Architecture

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                  â”ƒ Output Shape           â”ƒ       Param # â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Dense (64, ReLU)              â”‚ (None, 64)             â”‚         1,280 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dense (32, ReLU)              â”‚ (None, 32)             â”‚         2,080 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dropout (0.3)                 â”‚ (None, 32)             â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dense (16, ReLU)              â”‚ (None, 16)             â”‚           528 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dropout (0.2)                 â”‚ (None, 16)             â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dense (1, Sigmoid)            â”‚ (None, 1)              â”‚            17 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          Total Params:   3,905
```

### Training Configuration

| Setting | Detail |
|---|---|
| Optimizer | Adam |
| Loss | Binary Crossentropy |
| Regularization | Dropout (0.3, 0.2) |
| Weight Init | He Uniform |
| Overfitting Control | EarlyStopping (patience=5, monitor=val_loss) |
| Decision Threshold | 0.3 |
| Saved As | `deep_learning_model.keras` |

### ANN Test Set Performance

| Metric | No Churn | Churn |
|---|---|---|
| Precision | 0.89 | 0.49 |
| Recall | 0.71 | **0.76** |
| F1-Score | 0.79 | 0.60 |
| Accuracy | | **73%** |
| **AUC** | | **0.812** |

---

## ğŸ“Š Model Comparison

| Model | Accuracy | AUC | Churn Recall | Churn Precision |
|---|---|---|---|---|
| ğŸ¥‡ LightGBM | 75% | **0.834** | **0.78** | 0.52 |
| ğŸ¥ˆ ANN | 73% | 0.812 | 0.76 | 0.49 |

> Both models use a **0.3 decision threshold** to prioritize catching churners over precision.
> LightGBM edges out the ANN on all metrics while being significantly faster to train, making it the recommended production model.

---

## ğŸ–¥ï¸ Streamlit App

The final LightGBM model is deployed as an interactive web application built with Streamlit, allowing business users to get real-time churn predictions without any coding.

### Features

- ğŸ“ **Input Form** â€” Enter all 19 customer attributes via dropdowns and number inputs
- ğŸ¯ **Churn Probability** â€” Displays the model's predicted probability score
- ğŸš¦ **Risk Level Classification:**

| Risk Level | Probability Range | Indicator |
|---|---|---|
| ğŸŸ¢ Low Risk | < 0.3 | `st.success` |
| ğŸŸ¡ Medium Risk | 0.3 â€“ 0.6 | `st.warning` |
| ğŸ”´ High Risk | > 0.6 | `st.error` |

### How It Works

```
User fills in customer details (19 features)
              â†“
   Input encoded via pd.get_dummies()
              â†“
  Features aligned to training columns
              â†“
  LightGBM predicts churn probability
              â†“
  Threshold 0.3 applied â†’ Churn / No Churn
              â†“
     Risk level displayed to user
```

### Running the App

```bash
streamlit run deployment.py
```

---


## ğŸ“¦ Requirements

```txt
pandas
numpy
matplotlib
seaborn
scikit-learn
imbalanced-learn
xgboost
lightgbm
catboost
tensorflow
keras
streamlit
joblib
jupyter
```

> ğŸ’¡ To generate your exact pinned versions, run `pip freeze > requirements.txt` in your terminal.

---

## ğŸš€ How to Run

This project was developed using **GitHub Codespaces** â€” no local setup required!

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/Akhil-Ullas/Customer-Churn-Prediction)

```bash
# 1. Open the repository in GitHub Codespaces
#    Click the green "Code" button â†’ "Codespaces" tab â†’ "Create codespace on main"

# 2. Install dependencies
pip install -r requirements.txt

# 3. Add the dataset
# Place WA_Fn-UseC_-Telco-Customer-Churn.csv inside the /Data folder

# 4. Run ML notebook
jupyter notebook main.ipynb

# 5. Run Deep Learning notebook
jupyter notebook deep_learning.ipynb

# 6. Launch Streamlit App
streamlit run deployment.py
```

> ğŸ’¡ **Tip:** GitHub Codespaces provides a fully configured cloud environment with VS Code in the browser â€” no need to install Python, Jupyter, or any libraries locally.

---

## ğŸ“ Project Structure
```
CUSTOMER-CHURN-PREDICTION/
â”‚
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ cleaned_churn_data.csv
â”‚   â””â”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv
â”‚
â”œâ”€â”€ Deployment/
â”‚   â””â”€â”€ deployment.py            â† Streamlit web application
â”‚
â”œâ”€â”€ Models/
â”‚   â”œâ”€â”€ customer_churn_model.pkl
â”‚   â”œâ”€â”€ deep_learning_model.keras
â”‚   â””â”€â”€ feature_columns.pkl
â”‚
â”œâ”€â”€ PowerBI/
â”‚   â”œâ”€â”€ Deep Learning/
â”‚   â”œâ”€â”€ accuracy_metrics.csv
â”‚   â”œâ”€â”€ classification_report.csv
â”‚   â”œâ”€â”€ predictions.csv
â”‚   â””â”€â”€ training_data.csv
â”‚
â”œâ”€â”€ main.ipynb                   â† ML pipeline (LightGBM, RandomForest, XGBoost...)
â”œâ”€â”€ deep_learning.ipynb          â† ANN pipeline (TensorFlow/Keras)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“Œ Business Impact

Lowering churn by even a few percentage points can translate to millions in retained revenue. This project provides two complementary modeling approaches and a deployable web app, giving both data teams and business stakeholders the tools they need.

| Business Value | Detail |
|---|---|
| ğŸ¯ Early Detection | Flags at-risk customers before they cancel |
| ğŸ’° Cost Reduction | Retaining customers is cheaper than acquiring new ones |
| ğŸ“Š Actionable Segments | High-risk groups identified for targeted campaigns |
| ğŸ§  Dual Modeling | Both ML and DL approaches explored for robustness |
| ğŸ–¥ï¸ Streamlit App | Real-time predictions with risk level classification |
| ğŸ“ˆ Power BI Dashboard | Non-technical stakeholders can monitor churn risk in real time |
```

